{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importando librerias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerias Generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ernesto.silva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ernesto.silva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ernesto.silva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ernesto.silva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ernesto.silva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ernesto.silva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ernesto.silva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ernesto.silva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Analisis y manipulacion de datos\n",
    "import numpy as np # Tratamiento de matrices\n",
    "import matplotlib.pyplot as plt # Graficos\n",
    "import seaborn as sns # Visualizacion de datos\n",
    "\n",
    "import nltk # Procesamiento del lenguaje natural\n",
    "nltk.download('averaged_perceptron_tagger') # Etiquetar las palabras\n",
    "nltk.download('vader_lexicon') # Analisis de sentimiento\n",
    "nltk.download('wordnet') # Categorizacion de las palabras\n",
    "nltk.download('stopwords') # Quitar palabras comunes\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag # Clasificacion de palabras\n",
    "from nltk.corpus import stopwords # Eliminar palabras vacias\n",
    "from nltk.tokenize import WhitespaceTokenizer # Tokenizar\n",
    "from nltk.stem import WordNetLemmatizer # Lematizar\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # Lematizar\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # Analisis de sentimiento\n",
    "\n",
    "import nltk # Procesamiento del lenguaje natural\n",
    "nltk.download('averaged_perceptron_tagger') # Etiquetar las palabras\n",
    "nltk.download('vader_lexicon') # Analisis de sentimiento\n",
    "nltk.download('wordnet') # Categorizacion de las palabras\n",
    "nltk.download('stopwords') # Quitar palabras comunes\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag # Clasificacion de palabras\n",
    "from nltk.corpus import stopwords # Eliminar palabras vacias\n",
    "from nltk.tokenize import WhitespaceTokenizer # Tokenizar\n",
    "from nltk.stem import WordNetLemmatizer # Lematizar\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # Lematizar\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # Analisis de sentimiento\n",
    "\n",
    "from wordcloud import WordCloud # Nube de palabras\n",
    "import string # Operaciones de cadenas de caracteres\n",
    "from textblob import TextBlob # Procesamiento del lenguaje\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Codificacion de documentos, segun frecuenca de las palabras\n",
    "from sklearn.model_selection import train_test_split # Dividir los datos en entrenamiento y validacion\n",
    "from imblearn.over_sampling import SMOTE # Balanceo de los datos\n",
    "from sklearn.linear_model import LogisticRegression # Clasificador\n",
    "from sklearn.ensemble import RandomForestClassifier # Clasificador\n",
    "from sklearn.metrics import classification_report # Metricas para valoracion del modelo\n",
    "from sklearn.metrics import f1_score, confusion_matrix # Metricas para valoracion del modelo\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score # Metricas para valoracion del modelo\n",
    "#from sklearn.metrics import plot_confusion_matrix # Metricas para valoracion del modelo\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV # Ajuste de hiper-parametros\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings # Control de advertencias\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas(desc='Processing Dataframe') # Barra de progreso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../files/reddit_mobile_operators_peru_opinions5.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset:   3663\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamaño del dataset:  \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_author</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_created_utc</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>comment_created_utc</th>\n",
       "      <th>operadora</th>\n",
       "      <th>mobile_operator_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ahmaq3</td>\n",
       "      <td>InsaneJSK</td>\n",
       "      <td>eeftoum</td>\n",
       "      <td>bestmaokaina</td>\n",
       "      <td>mejor compania internet fijo</td>\n",
       "      <td>2019-01-19 14:00:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>acabo mudarme lima quisiera saber experiencia ...</td>\n",
       "      <td>t5_2qp9h</td>\n",
       "      <td>0.73</td>\n",
       "      <td>movistar  los demas hacen throttling luego de ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2019-01-19 14:03:47</td>\n",
       "      <td>movistar</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ahmaq3</td>\n",
       "      <td>InsaneJSK</td>\n",
       "      <td>eegc69q</td>\n",
       "      <td>morto00x</td>\n",
       "      <td>mejor compania internet fijo</td>\n",
       "      <td>2019-01-19 14:00:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>acabo mudarme lima quisiera saber experiencia ...</td>\n",
       "      <td>t5_2qp9h</td>\n",
       "      <td>0.73</td>\n",
       "      <td>depende mucho de tu zona porque el ancho de ba...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2019-01-19 17:30:52</td>\n",
       "      <td>claro</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ahmaq3</td>\n",
       "      <td>InsaneJSK</td>\n",
       "      <td>eewh9y7</td>\n",
       "      <td>NiMierda</td>\n",
       "      <td>mejor compania internet fijo</td>\n",
       "      <td>2019-01-19 14:00:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>acabo mudarme lima quisiera saber experiencia ...</td>\n",
       "      <td>t5_2qp9h</td>\n",
       "      <td>0.73</td>\n",
       "      <td>depende de la zona, yo tuve mala experiencia c...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-01-25 04:35:20</td>\n",
       "      <td>claro</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ahmaq3</td>\n",
       "      <td>InsaneJSK</td>\n",
       "      <td>eewh9y7</td>\n",
       "      <td>NiMierda</td>\n",
       "      <td>mejor compania internet fijo</td>\n",
       "      <td>2019-01-19 14:00:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>acabo mudarme lima quisiera saber experiencia ...</td>\n",
       "      <td>t5_2qp9h</td>\n",
       "      <td>0.73</td>\n",
       "      <td>depende de la zona, yo tuve mala experiencia c...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-01-25 04:35:20</td>\n",
       "      <td>movistar</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahmaq3</td>\n",
       "      <td>InsaneJSK</td>\n",
       "      <td>eefutn7</td>\n",
       "      <td>kambriel1</td>\n",
       "      <td>mejor compania internet fijo</td>\n",
       "      <td>2019-01-19 14:00:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>acabo mudarme lima quisiera saber experiencia ...</td>\n",
       "      <td>t5_2qp9h</td>\n",
       "      <td>0.73</td>\n",
       "      <td>dependiendo de la zona, el mejor creo que pued...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-19 14:18:09</td>\n",
       "      <td>claro</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id post_author comment_id comment_author                    post_title  \\\n",
       "0  ahmaq3   InsaneJSK    eeftoum   bestmaokaina  mejor compania internet fijo   \n",
       "1  ahmaq3   InsaneJSK    eegc69q       morto00x  mejor compania internet fijo   \n",
       "2  ahmaq3   InsaneJSK    eewh9y7       NiMierda  mejor compania internet fijo   \n",
       "3  ahmaq3   InsaneJSK    eewh9y7       NiMierda  mejor compania internet fijo   \n",
       "4  ahmaq3   InsaneJSK    eefutn7      kambriel1  mejor compania internet fijo   \n",
       "\n",
       "      post_created_utc link_flair_text  \\\n",
       "0  2019-01-19 14:00:59             NaN   \n",
       "1  2019-01-19 14:00:59             NaN   \n",
       "2  2019-01-19 14:00:59             NaN   \n",
       "3  2019-01-19 14:00:59             NaN   \n",
       "4  2019-01-19 14:00:59             NaN   \n",
       "\n",
       "                                            selftext subreddit  upvote_ratio  \\\n",
       "0  acabo mudarme lima quisiera saber experiencia ...  t5_2qp9h          0.73   \n",
       "1  acabo mudarme lima quisiera saber experiencia ...  t5_2qp9h          0.73   \n",
       "2  acabo mudarme lima quisiera saber experiencia ...  t5_2qp9h          0.73   \n",
       "3  acabo mudarme lima quisiera saber experiencia ...  t5_2qp9h          0.73   \n",
       "4  acabo mudarme lima quisiera saber experiencia ...  t5_2qp9h          0.73   \n",
       "\n",
       "                                             comment  comment_score  \\\n",
       "0  movistar  los demas hacen throttling luego de ...            3.0   \n",
       "1  depende mucho de tu zona porque el ancho de ba...            3.0   \n",
       "2  depende de la zona, yo tuve mala experiencia c...            2.0   \n",
       "3  depende de la zona, yo tuve mala experiencia c...            2.0   \n",
       "4  dependiendo de la zona, el mejor creo que pued...            1.0   \n",
       "\n",
       "   comment_created_utc operadora mobile_operator_sentiment  \n",
       "0  2019-01-19 14:03:47  movistar                    neutro  \n",
       "1  2019-01-19 17:30:52     claro                  positivo  \n",
       "2  2019-01-25 04:35:20     claro                  positivo  \n",
       "3  2019-01-25 04:35:20  movistar                  positivo  \n",
       "4  2019-01-19 14:18:09     claro                  positivo  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['post_id', 'post_author', 'comment_id', 'comment_author', 'post_title',\n",
       "       'post_created_utc', 'link_flair_text', 'selftext', 'subreddit',\n",
       "       'upvote_ratio', 'comment', 'comment_score', 'comment_created_utc',\n",
       "       'operadora', 'mobile_operator_sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Principales funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para limpiar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_texto(texto):\n",
    "    # Poner el texto en minúsculas\n",
    "    texto = texto.lower()\n",
    "    # Tokenizar el texto y quitar los signos de puntuación\n",
    "    texto = [word.strip(string.punctuation) for word in texto.split(\" \")]\n",
    "    # Quitar las palabras que contengan números\n",
    "    texto = [word for word in texto if not any(c.isdigit() for c in word)]\n",
    "    # Quitar las stop words\n",
    "    stop = stopwords.words('english')\n",
    "    texto = [x for x in texto if x not in stop]\n",
    "    # Quitar los tokens vacíos\n",
    "    texto = [t for t in texto if len(t) > 0]\n",
    "    # Pos tags\n",
    "    pos_tags = pos_tag(texto)\n",
    "    # Lematizar el texto\n",
    "    texto = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    # Quitar las palabras con sólo una letra\n",
    "    texto = [t for t in texto if len(t) > 1]\n",
    "    # Unir todo\n",
    "    texto = \" \".join(texto)\n",
    "    return(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para dibujar la nube de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color = 'white',\n",
    "        max_words = 200,\n",
    "        max_font_size = 40, \n",
    "        scale = 3,\n",
    "        random_state = 42\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize = (20, 20))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize = 20)\n",
    "        fig.subplots_adjust(top = 2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para el etiquetado de nombres, verbos, adjetivos o adverbios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para probar los modelos y mejorarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=101):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.loc[perm[:train_end]]\n",
    "    validate = df.loc[perm[train_end:validate_end]]\n",
    "    test = df.loc[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analisis exploratorio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
