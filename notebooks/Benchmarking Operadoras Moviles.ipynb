{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importando librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Matplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "from keras import utils\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "\n",
    "# Word2vec\n",
    "import gensim\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "# Set log\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../files/reddit_mobile_operators_peru_opinions_dataset.csv', header=0)\n",
    "df = df.dropna(subset=['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset:   5985\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamaño del dataset:  \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_cnt = Counter(df.sentiment)\n",
    "#plt.figure(figsize=(16,8))\n",
    "#plt.bar(target_cnt.keys(), target_cnt.values())\n",
    "#plt.title(\"Distribución de las etiquetas de los conjuntos de datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre procesando el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# TEXT CLENAING\n",
    "TEXT_CLEANING_RE = \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "stop_words = ['a','actualmente','adelante','además','afirmó','agregó','ahora','ahí','al','algo','alguna','algunas','alguno','algunos','algún','alrededor','ambos','ampleamos','ante','anterior','antes','apenas','aproximadamente','aquel','aquellas','aquellos','aqui','aquí','arriba','aseguró','así','atras','aunque','ayer','añadió','aún','bajo','bastante','bien','buen','buena','buenas','bueno','buenos','cada','casi','cerca','cierta','ciertas','cierto','ciertos','cinco','comentó','como','con','conocer','conseguimos','conseguir','considera','consideró','consigo','consigue','consiguen','consigues','contra','cosas','creo','cual','cuales','cualquier','cuando','cuanto','cuatro','cuenta','cómo','da','dado','dan','dar','de','debe','deben','debido','decir','dejó','del','demás','dentro','desde','después','dice','dicen','dicho','dieron','diferente','diferentes','dijeron','dijo','dio','donde','dos','durante','e','ejemplo','el','ella','ellas','ello','ellos','embargo','empleais','emplean','emplear','empleas','empleo','en','encima','encuentra','entonces','entre','era','erais','eramos','eran','eras','eres','es','esa','esas','ese','eso','esos','esta','estaba','estabais','estaban','estabas','estad','estada','estadas','estado','estados','estais','estamos','estan','estando','estar','estaremos','estará','estarán','estarás','estaré','estaréis','estaría','estaríais','estaríamos','estarían','estarías','estas','este','estemos','esto','estos','estoy','estuve','estuviera','estuvierais','estuvieran','estuvieras','estuvieron','estuviese','estuvieseis','estuviesen','estuvieses','estuvimos','estuviste','estuvisteis','estuviéramos','estuviésemos','estuvo','está','estábamos','estáis','están','estás','esté','estéis','estén','estés','ex','existe','existen','explicó','expresó','fin','fue','fuera','fuerais','fueran','fueras','fueron','fuese','fueseis','fuesen','fueses','fui','fuimos','fuiste','fuisteis','fuéramos','fuésemos','gran','grandes','gueno','ha','haber','habida','habidas','habido','habidos','habiendo','habremos','habrá','habrán','habrás','habré','habréis','habría','habríais','habríamos','habrían','habrías','habéis','había','habíais','habíamos','habían','habías','hace','haceis','hacemos','hacen','hacer','hacerlo','haces','hacia','haciendo','hago','han','has','hasta','hay','haya','hayamos','hayan','hayas','hayáis','he','hecho','hemos','hicieron','hizo','hoy','hube','hubiera','hubierais','hubieran','hubieras','hubieron','hubiese','hubieseis','hubiesen','hubieses','hubimos','hubiste','hubisteis','hubiéramos','hubiésemos','hubo','igual','incluso','indicó','informó','intenta','intentais','intentamos','intentan','intentar','intentas','intento','ir','junto','la','lado','largo','las','le','les','llegó','lleva','llevar','lo','los','luego','lugar','manera','manifestó','mayor','me','mediante','mejor','mencionó','menos','mi','mientras','mio','mis','misma','mismas','mismo','mismos','modo','momento','mucha','muchas','mucho','muchos','muy','más','mí','mía','mías','mío','míos','nada','nadie','ni','ninguna','ningunas','ninguno','ningunos','ningún','no','nos','nosotras','nosotros','nuestra','nuestras','nuestro','nuestros','nueva','nuevas','nuevo','nuevos','nunca','o','ocho','os','otra','otras','otro','otros','para','parece','parte','partir','pasada','pasado','pero','pesar','poca','pocas','poco','pocos','podeis','podemos','poder','podria','podriais','podriamos','podrian','podrias','podrá','podrán','podría','podrían','poner','por','por qué','porque','posible','primer','primera','primero','primeros','principalmente','propia','propias','propio','propios','próximo','próximos','pudo','pueda','puede','pueden','puedo','pues','que','quedó','queremos','quien','quienes','quiere','quién','qué','realizado','realizar','realizó','respecto','sabe','sabeis','sabemos','saben','saber','sabes','se','sea','seamos','sean','seas','segunda','segundo','según','seis','ser','seremos','será','serán','serás','seré','seréis','sería','seríais','seríamos','serían','serías','seáis','señaló','si','sido','siempre','siendo','siete','sigue','siguiente','sin','sino','sobre','sois','sola','solamente','solas','solo','solos','somos','son','soy','su','sus','suya','suyas','suyo','suyos','sí','sólo','tal','también','tampoco','tan','tanto','te','tendremos','tendrá','tendrán','tendrás','tendré','tendréis','tendría','tendríais','tendríamos','tendrían','tendrías','tened','teneis','tenemos','tener','tenga','tengamos','tengan','tengas','tengo','tengáis','tenida','tenidas','tenido','tenidos','teniendo','tenéis','tenía','teníais','teníamos','tenían','tenías','tercera','ti','tiempo','tiene','tienen','tienes','toda','todas','todavía','todo','todos','total','trabaja','trabajais','trabajamos','trabajan','trabajar','trabajas','trabajo','tras','trata','través','tres','tu','tus','tuve','tuviera','tuvierais','tuvieran','tuvieras','tuvieron','tuviese','tuvieseis','tuviesen','tuvieses','tuvimos','tuviste','tuvisteis','tuviéramos','tuviésemos','tuvo','tuya','tuyas','tuyo','tuyos','tú','ultimo','un','una','unas','uno','unos','usa','usais','usamos','usan','usar','usas','uso','usted','va','vais','valor','vamos','van','varias','varios','vaya','veces','ver','verdad','verdadera','VERDADERO','vez','vosotras','vosotros','voy','vuestra','vuestras','vuestro','vuestros','y','ya','yo','él','éramos','ésta','éstas','éste','éstos','última','últimas','último','últimos','sintiendo','sentido','sentida','sentidos','sentidas','siente','sentid',]\n",
    "#stop_words = stopwords.words(\"spanish\")\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', str(text).lower()).strip()\n",
    "    text = re.sub('@[^\\s]+', ' ', text)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.translate(str.maketrans('', '', '1234567890'))\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "#\n",
    "#print('hola' in stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def preprocess(text, stem=False):\n",
    "#    # Remove link,user and special characters\n",
    "#    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "#    tokens = []\n",
    "#    for token in text.split():\n",
    "#        if token not in stop_words:\n",
    "#            tokens.append(token)\n",
    "#            #if stem:\n",
    "#            #    tokens.append(stemmer.stem(token))\n",
    "#            #else:\n",
    "#            #    tokens.append(token)\n",
    "#    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 812 ms\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "df['comments_preprocess'] = df.comment.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividir la formación y las pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN size: 4788\n",
      "TEST size: 1197\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\n",
    "print(\"TRAIN size:\", len(df_train))\n",
    "print(\"TEST size:\", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asigna df_train.txt a 'documentos'\n",
    "documentos = df_train.comments_preprocess\n",
    "\n",
    "# Elimina los duplicados\n",
    "documentos.drop_duplicates(inplace=True)\n",
    "\n",
    "# Aplica 'split' a cada documento\n",
    "documentos = [texto.split() for texto in documentos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 11:54:17,622 : INFO : collecting all words and their counts\n",
      "2023-11-15 11:54:17,623 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-11-15 11:54:17,639 : INFO : collected 16736 word types from a corpus of 70708 raw words and 4627 sentences\n",
      "2023-11-15 11:54:17,640 : INFO : Creating a fresh vocabulary\n",
      "2023-11-15 11:54:17,648 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 1405 unique words (8.40% of original 16736, drops 15331)', 'datetime': '2023-11-15T11:54:17.648147', 'gensim': '4.3.2', 'python': '3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.23585-SP0', 'event': 'prepare_vocab'}\n",
      "2023-11-15 11:54:17,649 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 39784 word corpus (56.27% of original 70708, drops 30924)', 'datetime': '2023-11-15T11:54:17.649151', 'gensim': '4.3.2', 'python': '3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.23585-SP0', 'event': 'prepare_vocab'}\n",
      "2023-11-15 11:54:17,655 : INFO : deleting the raw counts dictionary of 16736 items\n",
      "2023-11-15 11:54:17,656 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2023-11-15 11:54:17,656 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 37208.40505228525 word corpus (93.5%% of prior 39784)', 'datetime': '2023-11-15T11:54:17.656151', 'gensim': '4.3.2', 'python': '3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.23585-SP0', 'event': 'prepare_vocab'}\n",
      "2023-11-15 11:54:17,668 : INFO : estimated required memory for 1405 words and 400 dimensions: 5198500 bytes\n",
      "2023-11-15 11:54:17,668 : INFO : resetting layer weights\n",
      "2023-11-15 11:54:17,671 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-11-15T11:54:17.671149', 'gensim': '4.3.2', 'python': '3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.23585-SP0', 'event': 'build_vocab'}\n",
      "2023-11-15 11:54:17,671 : INFO : Word2Vec lifecycle event {'msg': 'training model with 12 workers on 1405 vocabulary and 400 features, using sg=1 hs=0 sample=0.001 negative=5 window=12 shrink_windows=True', 'datetime': '2023-11-15T11:54:17.671149', 'gensim': '4.3.2', 'python': '3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.23585-SP0', 'event': 'train'}\n",
      "2023-11-15 11:54:17,800 : INFO : EPOCH 0: training on 70708 raw words (37203 effective words) took 0.1s, 306764 effective words/s\n",
      "2023-11-15 11:54:17,928 : INFO : EPOCH 1: training on 70708 raw words (37258 effective words) took 0.1s, 309716 effective words/s\n",
      "2023-11-15 11:54:18,054 : INFO : EPOCH 2: training on 70708 raw words (37212 effective words) took 0.1s, 313743 effective words/s\n",
      "2023-11-15 11:54:18,179 : INFO : EPOCH 3: training on 70708 raw words (37219 effective words) took 0.1s, 314985 effective words/s\n",
      "2023-11-15 11:54:18,306 : INFO : EPOCH 4: training on 70708 raw words (37106 effective words) took 0.1s, 313339 effective words/s\n",
      "2023-11-15 11:54:18,435 : INFO : EPOCH 5: training on 70708 raw words (37210 effective words) took 0.1s, 305946 effective words/s\n",
      "2023-11-15 11:54:18,562 : INFO : EPOCH 6: training on 70708 raw words (37237 effective words) took 0.1s, 311806 effective words/s\n",
      "2023-11-15 11:54:18,690 : INFO : EPOCH 7: training on 70708 raw words (37238 effective words) took 0.1s, 312666 effective words/s\n",
      "2023-11-15 11:54:18,815 : INFO : EPOCH 8: training on 70708 raw words (37185 effective words) took 0.1s, 312869 effective words/s\n",
      "2023-11-15 11:54:18,947 : INFO : EPOCH 9: training on 70708 raw words (37164 effective words) took 0.1s, 302432 effective words/s\n",
      "2023-11-15 11:54:19,070 : INFO : EPOCH 10: training on 70708 raw words (37210 effective words) took 0.1s, 317999 effective words/s\n",
      "2023-11-15 11:54:19,197 : INFO : EPOCH 11: training on 70708 raw words (37192 effective words) took 0.1s, 313082 effective words/s\n",
      "2023-11-15 11:54:19,323 : INFO : EPOCH 12: training on 70708 raw words (37214 effective words) took 0.1s, 312875 effective words/s\n",
      "2023-11-15 11:54:19,456 : INFO : EPOCH 13: training on 70708 raw words (37219 effective words) took 0.1s, 300593 effective words/s\n",
      "2023-11-15 11:54:19,580 : INFO : EPOCH 14: training on 70708 raw words (37195 effective words) took 0.1s, 320602 effective words/s\n",
      "2023-11-15 11:54:19,712 : INFO : EPOCH 15: training on 70708 raw words (37146 effective words) took 0.1s, 297645 effective words/s\n",
      "2023-11-15 11:54:19,844 : INFO : EPOCH 16: training on 70708 raw words (37187 effective words) took 0.1s, 300167 effective words/s\n",
      "2023-11-15 11:54:19,973 : INFO : EPOCH 17: training on 70708 raw words (37249 effective words) took 0.1s, 310290 effective words/s\n",
      "2023-11-15 11:54:20,100 : INFO : EPOCH 18: training on 70708 raw words (37185 effective words) took 0.1s, 310044 effective words/s\n",
      "2023-11-15 11:54:20,237 : INFO : EPOCH 19: training on 70708 raw words (37172 effective words) took 0.1s, 288761 effective words/s\n",
      "2023-11-15 11:54:20,237 : INFO : Word2Vec lifecycle event {'msg': 'training on 1414160 raw words (744001 effective words) took 2.6s, 289916 effective words/s', 'datetime': '2023-11-15T11:54:20.237827', 'gensim': '4.3.2', 'python': '3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.23585-SP0', 'event': 'train'}\n",
      "2023-11-15 11:54:20,238 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1405, vector_size=400, alpha=0.025>', 'datetime': '2023-11-15T11:54:20.238829', 'gensim': '4.3.2', 'python': '3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.23585-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "## WORD2VEC \n",
    "#W2V_SIZE = 300\n",
    "#W2V_WINDOW = 15\n",
    "#W2V_EPOCH = 32\n",
    "#W2V_MIN_COUNT = 6\n",
    "\n",
    "# WORD2VEC \n",
    "W2V_SIZE = 400\n",
    "W2V_WINDOW = 12\n",
    "W2V_EPOCH = 75\n",
    "W2V_MIN_COUNT = 10\n",
    "\n",
    "#\n",
    "#\n",
    "#w2v_model = gensim.models.word2vec.Word2Vec(vector_size=W2V_SIZE, \n",
    "#                                            window=W2V_WINDOW, \n",
    "#                                            min_count=W2V_MIN_COUNT, \n",
    "#                                            workers=8)\n",
    "import multiprocessing\n",
    "hhgroups2vec = gensim.models.word2vec.Word2Vec(\n",
    "    documentos,\n",
    "    sg=1,\n",
    "    seed=1,\n",
    "    workers=multiprocessing.cpu_count(),\n",
    "    vector_size=W2V_SIZE,\n",
    "    min_count=W2V_MIN_COUNT,\n",
    "    window=W2V_WINDOW,    epochs=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 11:54:20,246 : INFO : collecting all words and their counts\n",
      "2023-11-15 11:54:20,247 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-11-15 11:54:20,264 : INFO : collected 16736 word types from a corpus of 70708 raw words and 4627 sentences\n",
      "2023-11-15 11:54:20,265 : INFO : Creating a fresh vocabulary\n",
      "2023-11-15 11:54:20,272 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 1405 unique words (8.40% of original 16736, drops 15331)', 'datetime': '2023-11-15T11:54:20.272066', 'gensim': '4.3.2', 'python': '3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.23585-SP0', 'event': 'prepare_vocab'}\n",
      "2023-11-15 11:54:20,273 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 39784 word corpus (56.27% of original 70708, drops 30924)', 'datetime': '2023-11-15T11:54:20.273065', 'gensim': '4.3.2', 'python': '3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.23585-SP0', 'event': 'prepare_vocab'}\n",
      "2023-11-15 11:54:20,281 : INFO : deleting the raw counts dictionary of 16736 items\n",
      "2023-11-15 11:54:20,282 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2023-11-15 11:54:20,283 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 37208.40505228525 word corpus (93.5%% of prior 39784)', 'datetime': '2023-11-15T11:54:20.283063', 'gensim': '4.3.2', 'python': '3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.23585-SP0', 'event': 'prepare_vocab'}\n",
      "2023-11-15 11:54:20,284 : WARNING : sorting after vectors have been allocated is expensive & error-prone\n",
      "2023-11-15 11:54:20,295 : INFO : estimated required memory for 1405 words and 400 dimensions: 5198500 bytes\n",
      "2023-11-15 11:54:20,297 : INFO : resetting layer weights\n",
      "2023-11-15 11:54:20,297 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-11-15T11:54:20.297064', 'gensim': '4.3.2', 'python': '3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.23585-SP0', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "hhgroups2vec.build_vocab(documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 1405\n"
     ]
    }
   ],
   "source": [
    "words = list(hhgroups2vec.wv.key_to_index)\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 11:54:20,317 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2023-11-15 11:54:20,318 : INFO : Word2Vec lifecycle event {'msg': 'training model with 12 workers on 1405 vocabulary and 400 features, using sg=1 hs=0 sample=0.001 negative=5 window=12 shrink_windows=True', 'datetime': '2023-11-15T11:54:20.318457', 'gensim': '4.3.2', 'python': '3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.23585-SP0', 'event': 'train'}\n",
      "2023-11-15 11:54:20,465 : INFO : EPOCH 0: training on 70708 raw words (37267 effective words) took 0.1s, 275945 effective words/s\n",
      "2023-11-15 11:54:20,590 : INFO : EPOCH 1: training on 70708 raw words (37168 effective words) took 0.1s, 318896 effective words/s\n",
      "2023-11-15 11:54:20,714 : INFO : EPOCH 2: training on 70708 raw words (37195 effective words) took 0.1s, 321835 effective words/s\n",
      "2023-11-15 11:54:20,841 : INFO : EPOCH 3: training on 70708 raw words (37185 effective words) took 0.1s, 309052 effective words/s\n",
      "2023-11-15 11:54:20,976 : INFO : EPOCH 4: training on 70708 raw words (37146 effective words) took 0.1s, 293104 effective words/s\n",
      "2023-11-15 11:54:21,106 : INFO : EPOCH 5: training on 70708 raw words (37233 effective words) took 0.1s, 306328 effective words/s\n",
      "2023-11-15 11:54:21,231 : INFO : EPOCH 6: training on 70708 raw words (37237 effective words) took 0.1s, 316586 effective words/s\n",
      "2023-11-15 11:54:21,357 : INFO : EPOCH 7: training on 70708 raw words (37207 effective words) took 0.1s, 316276 effective words/s\n",
      "2023-11-15 11:54:21,486 : INFO : EPOCH 8: training on 70708 raw words (37168 effective words) took 0.1s, 307481 effective words/s\n",
      "2023-11-15 11:54:21,610 : INFO : EPOCH 9: training on 70708 raw words (37216 effective words) took 0.1s, 317320 effective words/s\n",
      "2023-11-15 11:54:21,734 : INFO : EPOCH 10: training on 70708 raw words (37228 effective words) took 0.1s, 319818 effective words/s\n",
      "2023-11-15 11:54:21,865 : INFO : EPOCH 11: training on 70708 raw words (37207 effective words) took 0.1s, 303797 effective words/s\n",
      "2023-11-15 11:54:21,998 : INFO : EPOCH 12: training on 70708 raw words (37235 effective words) took 0.1s, 294129 effective words/s\n",
      "2023-11-15 11:54:22,132 : INFO : EPOCH 13: training on 70708 raw words (37283 effective words) took 0.1s, 298583 effective words/s\n",
      "2023-11-15 11:54:22,266 : INFO : EPOCH 14: training on 70708 raw words (37191 effective words) took 0.1s, 294504 effective words/s\n",
      "2023-11-15 11:54:22,401 : INFO : EPOCH 15: training on 70708 raw words (37220 effective words) took 0.1s, 291633 effective words/s\n",
      "2023-11-15 11:54:22,535 : INFO : EPOCH 16: training on 70708 raw words (37313 effective words) took 0.1s, 295358 effective words/s\n",
      "2023-11-15 11:54:22,667 : INFO : EPOCH 17: training on 70708 raw words (37227 effective words) took 0.1s, 300190 effective words/s\n",
      "2023-11-15 11:54:22,795 : INFO : EPOCH 18: training on 70708 raw words (37277 effective words) took 0.1s, 308843 effective words/s\n",
      "2023-11-15 11:54:22,922 : INFO : EPOCH 19: training on 70708 raw words (37215 effective words) took 0.1s, 313748 effective words/s\n",
      "2023-11-15 11:54:23,048 : INFO : EPOCH 20: training on 70708 raw words (37207 effective words) took 0.1s, 318295 effective words/s\n",
      "2023-11-15 11:54:23,184 : INFO : EPOCH 21: training on 70708 raw words (37248 effective words) took 0.1s, 286990 effective words/s\n",
      "2023-11-15 11:54:23,311 : INFO : EPOCH 22: training on 70708 raw words (37179 effective words) took 0.1s, 317666 effective words/s\n",
      "2023-11-15 11:54:23,434 : INFO : EPOCH 23: training on 70708 raw words (37230 effective words) took 0.1s, 321255 effective words/s\n",
      "2023-11-15 11:54:23,560 : INFO : EPOCH 24: training on 70708 raw words (37202 effective words) took 0.1s, 312586 effective words/s\n",
      "2023-11-15 11:54:23,676 : INFO : EPOCH 25: training on 70708 raw words (37160 effective words) took 0.1s, 341741 effective words/s\n",
      "2023-11-15 11:54:23,797 : INFO : EPOCH 26: training on 70708 raw words (37218 effective words) took 0.1s, 331627 effective words/s\n",
      "2023-11-15 11:54:23,926 : INFO : EPOCH 27: training on 70708 raw words (37165 effective words) took 0.1s, 304789 effective words/s\n",
      "2023-11-15 11:54:24,050 : INFO : EPOCH 28: training on 70708 raw words (37213 effective words) took 0.1s, 321567 effective words/s\n",
      "2023-11-15 11:54:24,170 : INFO : EPOCH 29: training on 70708 raw words (37171 effective words) took 0.1s, 329755 effective words/s\n",
      "2023-11-15 11:54:24,293 : INFO : EPOCH 30: training on 70708 raw words (37186 effective words) took 0.1s, 322396 effective words/s\n",
      "2023-11-15 11:54:24,407 : INFO : EPOCH 31: training on 70708 raw words (37183 effective words) took 0.1s, 349748 effective words/s\n",
      "2023-11-15 11:54:24,530 : INFO : EPOCH 32: training on 70708 raw words (37180 effective words) took 0.1s, 324021 effective words/s\n",
      "2023-11-15 11:54:24,652 : INFO : EPOCH 33: training on 70708 raw words (37253 effective words) took 0.1s, 322331 effective words/s\n",
      "2023-11-15 11:54:24,774 : INFO : EPOCH 34: training on 70708 raw words (37214 effective words) took 0.1s, 324395 effective words/s\n",
      "2023-11-15 11:54:24,900 : INFO : EPOCH 35: training on 70708 raw words (37196 effective words) took 0.1s, 318280 effective words/s\n",
      "2023-11-15 11:54:25,018 : INFO : EPOCH 36: training on 70708 raw words (37130 effective words) took 0.1s, 335095 effective words/s\n",
      "2023-11-15 11:54:25,144 : INFO : EPOCH 37: training on 70708 raw words (37244 effective words) took 0.1s, 312429 effective words/s\n",
      "2023-11-15 11:54:25,267 : INFO : EPOCH 38: training on 70708 raw words (37208 effective words) took 0.1s, 322983 effective words/s\n",
      "2023-11-15 11:54:25,392 : INFO : EPOCH 39: training on 70708 raw words (37226 effective words) took 0.1s, 315701 effective words/s\n",
      "2023-11-15 11:54:25,514 : INFO : EPOCH 40: training on 70708 raw words (37202 effective words) took 0.1s, 328772 effective words/s\n",
      "2023-11-15 11:54:25,636 : INFO : EPOCH 41: training on 70708 raw words (37194 effective words) took 0.1s, 324303 effective words/s\n",
      "2023-11-15 11:54:25,761 : INFO : EPOCH 42: training on 70708 raw words (37218 effective words) took 0.1s, 314151 effective words/s\n",
      "2023-11-15 11:54:25,882 : INFO : EPOCH 43: training on 70708 raw words (37209 effective words) took 0.1s, 329561 effective words/s\n",
      "2023-11-15 11:54:26,003 : INFO : EPOCH 44: training on 70708 raw words (37179 effective words) took 0.1s, 324354 effective words/s\n",
      "2023-11-15 11:54:26,124 : INFO : EPOCH 45: training on 70708 raw words (37110 effective words) took 0.1s, 328865 effective words/s\n",
      "2023-11-15 11:54:26,246 : INFO : EPOCH 46: training on 70708 raw words (37246 effective words) took 0.1s, 326857 effective words/s\n",
      "2023-11-15 11:54:26,371 : INFO : EPOCH 47: training on 70708 raw words (37273 effective words) took 0.1s, 317892 effective words/s\n",
      "2023-11-15 11:54:26,491 : INFO : EPOCH 48: training on 70708 raw words (37278 effective words) took 0.1s, 330210 effective words/s\n",
      "2023-11-15 11:54:26,612 : INFO : EPOCH 49: training on 70708 raw words (37194 effective words) took 0.1s, 331928 effective words/s\n",
      "2023-11-15 11:54:26,735 : INFO : EPOCH 50: training on 70708 raw words (37207 effective words) took 0.1s, 321837 effective words/s\n",
      "2023-11-15 11:54:26,875 : INFO : EPOCH 51: training on 70708 raw words (37232 effective words) took 0.1s, 281123 effective words/s\n",
      "2023-11-15 11:54:27,014 : INFO : EPOCH 52: training on 70708 raw words (37208 effective words) took 0.1s, 286746 effective words/s\n",
      "2023-11-15 11:54:27,132 : INFO : EPOCH 53: training on 70708 raw words (37159 effective words) took 0.1s, 332970 effective words/s\n",
      "2023-11-15 11:54:27,250 : INFO : EPOCH 54: training on 70708 raw words (37187 effective words) took 0.1s, 335255 effective words/s\n",
      "2023-11-15 11:54:27,370 : INFO : EPOCH 55: training on 70708 raw words (37236 effective words) took 0.1s, 330409 effective words/s\n",
      "2023-11-15 11:54:27,489 : INFO : EPOCH 56: training on 70708 raw words (37233 effective words) took 0.1s, 333964 effective words/s\n",
      "2023-11-15 11:54:27,605 : INFO : EPOCH 57: training on 70708 raw words (37107 effective words) took 0.1s, 343759 effective words/s\n",
      "2023-11-15 11:54:27,719 : INFO : EPOCH 58: training on 70708 raw words (37183 effective words) took 0.1s, 347827 effective words/s\n",
      "2023-11-15 11:54:27,839 : INFO : EPOCH 59: training on 70708 raw words (37156 effective words) took 0.1s, 327578 effective words/s\n",
      "2023-11-15 11:54:27,960 : INFO : EPOCH 60: training on 70708 raw words (37153 effective words) took 0.1s, 332007 effective words/s\n",
      "2023-11-15 11:54:28,077 : INFO : EPOCH 61: training on 70708 raw words (37280 effective words) took 0.1s, 338389 effective words/s\n",
      "2023-11-15 11:54:28,197 : INFO : EPOCH 62: training on 70708 raw words (37203 effective words) took 0.1s, 332554 effective words/s\n",
      "2023-11-15 11:54:28,315 : INFO : EPOCH 63: training on 70708 raw words (37186 effective words) took 0.1s, 335610 effective words/s\n",
      "2023-11-15 11:54:28,434 : INFO : EPOCH 64: training on 70708 raw words (37205 effective words) took 0.1s, 332007 effective words/s\n",
      "2023-11-15 11:54:28,562 : INFO : EPOCH 65: training on 70708 raw words (37205 effective words) took 0.1s, 307955 effective words/s\n",
      "2023-11-15 11:54:28,681 : INFO : EPOCH 66: training on 70708 raw words (37186 effective words) took 0.1s, 335217 effective words/s\n",
      "2023-11-15 11:54:28,802 : INFO : EPOCH 67: training on 70708 raw words (37137 effective words) took 0.1s, 324601 effective words/s\n",
      "2023-11-15 11:54:28,921 : INFO : EPOCH 68: training on 70708 raw words (37235 effective words) took 0.1s, 335688 effective words/s\n",
      "2023-11-15 11:54:29,036 : INFO : EPOCH 69: training on 70708 raw words (37191 effective words) took 0.1s, 349082 effective words/s\n",
      "2023-11-15 11:54:29,158 : INFO : EPOCH 70: training on 70708 raw words (37219 effective words) took 0.1s, 323218 effective words/s\n",
      "2023-11-15 11:54:29,283 : INFO : EPOCH 71: training on 70708 raw words (37161 effective words) took 0.1s, 319105 effective words/s\n",
      "2023-11-15 11:54:29,400 : INFO : EPOCH 72: training on 70708 raw words (37196 effective words) took 0.1s, 335727 effective words/s\n",
      "2023-11-15 11:54:29,513 : INFO : EPOCH 73: training on 70708 raw words (37229 effective words) took 0.1s, 356109 effective words/s\n",
      "2023-11-15 11:54:29,628 : INFO : EPOCH 74: training on 70708 raw words (37233 effective words) took 0.1s, 344251 effective words/s\n",
      "2023-11-15 11:54:29,629 : INFO : Word2Vec lifecycle event {'msg': 'training on 5303100 raw words (2790461 effective words) took 9.3s, 299705 effective words/s', 'datetime': '2023-11-15T11:54:29.629688', 'gensim': '4.3.2', 'python': '3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.23585-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 33 s\n",
      "Wall time: 9.31 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2790461, 5303100)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "hhgroups2vec.train(documentos, total_examples=len(documentos), epochs=W2V_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('muchísimo', 0.321259081363678),\n",
       " ('principal', 0.29011350870132446),\n",
       " ('atencion', 0.288583904504776),\n",
       " ('momentos', 0.2787383198738098),\n",
       " ('madre', 0.2775704860687256),\n",
       " ('xddd', 0.27683714032173157),\n",
       " ('español', 0.27352607250213623),\n",
       " ('soporte', 0.2635883092880249),\n",
       " ('dios', 0.26189321279525757),\n",
       " ('estabilidad', 0.2603915333747864)]"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hhgroups2vec.wv.most_similar(\"excelente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'bien' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Documentos\\University\\Tesis\\2023-2\\benchmarking_mobile_operators\\notebooks\\Benchmarking Operadoras Moviles.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documentos/University/Tesis/2023-2/benchmarking_mobile_operators/notebooks/Benchmarking%20Operadoras%20Moviles.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m hhgroups2vec\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mmost_similar(\u001b[39m\"\u001b[39;49m\u001b[39mbien\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Documentos\\University\\Tesis\\2023-2\\benchmarking_mobile_operators\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[39m=\u001b[39m item[\u001b[39m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[39m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_mean_vector(keys, weight, pre_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, post_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, ignore_missing\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    842\u001b[0m all_keys \u001b[39m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_index(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, _KEY_TYPES) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[39mif\u001b[39;00m indexer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(topn, \u001b[39mint\u001b[39m):\n",
      "File \u001b[1;32md:\\Documentos\\University\\Tesis\\2023-2\\benchmarking_mobile_operators\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present in vocabulary\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[39mif\u001b[39;00m total_weight \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[39m=\u001b[39m mean \u001b[39m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'bien' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "hhgroups2vec.wv.most_similar(\"bien\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'bien' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Documentos\\University\\Tesis\\2023-2\\benchmarking_mobile_operators\\notebooks\\Benchmarking Operadoras Moviles.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documentos/University/Tesis/2023-2/benchmarking_mobile_operators/notebooks/Benchmarking%20Operadoras%20Moviles.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m hhgroups2vec\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mmost_similar(\u001b[39m\"\u001b[39;49m\u001b[39mbien\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Documentos\\University\\Tesis\\2023-2\\benchmarking_mobile_operators\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[39m=\u001b[39m item[\u001b[39m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[39m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_mean_vector(keys, weight, pre_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, post_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, ignore_missing\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    842\u001b[0m all_keys \u001b[39m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_index(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, _KEY_TYPES) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[39mif\u001b[39;00m indexer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(topn, \u001b[39mint\u001b[39m):\n",
      "File \u001b[1;32md:\\Documentos\\University\\Tesis\\2023-2\\benchmarking_mobile_operators\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present in vocabulary\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[39mif\u001b[39;00m total_weight \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[39m=\u001b[39m mean \u001b[39m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'bien' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "hhgroups2vec.wv.most_similar(\"bien\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inmediato', 0.39453625679016113),\n",
       " ('veas', 0.37086236476898193),\n",
       " ('oro', 0.359712153673172),\n",
       " ('simétrica', 0.3575632870197296),\n",
       " ('contraté', 0.34624508023262024),\n",
       " ('dijiste', 0.3446570932865143),\n",
       " ('crecimiento', 0.3371012806892395),\n",
       " ('robistar', 0.3337101340293884),\n",
       " ('proyectos', 0.3298235833644867),\n",
       " ('comunidades', 0.3270324468612671)]"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hhgroups2vec.wv.most_similar(\"excelente\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
